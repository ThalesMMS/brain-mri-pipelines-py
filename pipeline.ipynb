{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "31db2213",
      "metadata": {},
      "source": [
        "\n",
        "# Brain MRI Pipelines \u2014 notebook execut\u00e1vel (sem Tkinter)\n",
        "\n",
        "Este notebook reorganiza o c\u00f3digo do app em Tkinter em um fluxo que pode ser executado diretamente: segmenta\u00e7\u00e3o dos ventr\u00edculos, cria\u00e7\u00e3o do dataset combinado e treinos (SVM, XGBoost e DenseNet). As sa\u00eddas s\u00e3o gravadas em `output/` e os passos podem ser executados de forma independente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6003f0e",
      "metadata": {},
      "source": [
        "\n",
        "## Fluxo sugerido\n",
        "1) Configurar caminhos e depend\u00eancias.  \n",
        "2) (Opcional) Processar/segmentar as imagens NIfTI em lote.  \n",
        "3) Gerar o dataset `exam_level_dataset_split.csv` combinando descritores com o CSV demogr\u00e1fico.  \n",
        "4) Treinar modelos cl\u00e1ssicos (SVM / XGBoost).  \n",
        "5) Treinar DenseNet (classifica\u00e7\u00e3o ou regress\u00e3o).  \n",
        "\n",
        "Todas as fun\u00e7\u00f5es aqui dentro evitam qualquer chamada ao Tkinter e usam apenas c\u00f3digo headless.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c716a97",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Notebook auto-notes: keep split consistent; original_path rebuilt in code.\n",
        "# Configura caminhos base e valida se dataset/output e CSVs est\u00e3o presentes.\n",
        "\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "BASE_DIR = Path.cwd()\n",
        "DATASET_DIR = BASE_DIR / \"axl\"\n",
        "OUTPUT_DIR = BASE_DIR / \"output\"\n",
        "NOT_VIABLE_DIR = BASE_DIR / \"not_viable\"\n",
        "CSV_DEMOGRAPHIC = BASE_DIR / \"oasis_longitudinal_demographic.csv\"\n",
        "DESCRIPTORS_CSV = OUTPUT_DIR / \"ventricle_descriptors.csv\"\n",
        "EXAM_SPLIT_CSV = OUTPUT_DIR / \"exam_level_dataset_split.csv\"\n",
        "HISTORY_JSON = OUTPUT_DIR / \"training_experiments.json\"\n",
        "\n",
        "for p in (OUTPUT_DIR, NOT_VIABLE_DIR):\n",
        "    p.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Base: {BASE_DIR}\")\n",
        "print(f\"Dataset: {DATASET_DIR.exists()} | Arquivos NIfTI: {len(list(DATASET_DIR.glob('*.nii*')))}\")\n",
        "print(f\"CSV demogr\u00e1fico: {CSV_DEMOGRAPHIC.exists()}\")\n",
        "print(f\"Descritores existentes: {DESCRIPTORS_CSV.exists()}\")\n",
        "print(f\"Split existente: {EXAM_SPLIT_CSV.exists()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dda2eec4",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from brain_mri.utils.dependencies import ensure_dependencies\n",
        "\n",
        "missing = ensure_dependencies(BASE_DIR / \"requirements.txt\")\n",
        "if missing:\n",
        "    print(\"Ainda faltam pacotes para instalar manualmente:\", \", \".join(missing))\n",
        "else:\n",
        "    print(\"Depend\u00eancias principais atendidas.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a12c993",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "nii_files = sorted(DATASET_DIR.glob('*.nii*'))\n",
        "print(f\"Total de exames NIfTI encontrados: {len(nii_files)}\")\n",
        "if nii_files:\n",
        "    print(\"Exemplos:\")\n",
        "    for f in nii_files[:3]:\n",
        "        print(\" -\", f.name)\n",
        "\n",
        "if DESCRIPTORS_CSV.exists():\n",
        "    df_desc = pd.read_csv(DESCRIPTORS_CSV)\n",
        "    print(f\"Descritores: {len(df_desc)} linhas, colunas: {list(df_desc.columns)[:8]} ...\")\n",
        "if EXAM_SPLIT_CSV.exists():\n",
        "    df_split = pd.read_csv(EXAM_SPLIT_CSV)\n",
        "    display(df_split.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db126217",
      "metadata": {},
      "source": [
        "## Fun\u00e7\u00f5es utilit\u00e1rias (sem Tkinter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60f6868f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Notebook auto-notes: keep split consistent; original_path rebuilt in code.\n",
        "# Utilidades compartilhadas: bootstrap de depend\u00eancias, helpers de logging/plot,\n",
        "# resolu\u00e7\u00e3o de caminhos originais e c\u00e1lculo longitudinal para descritores.\n",
        "\n",
        "import json\n",
        "import re\n",
        "import shutil\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "try:\n",
        "    import nibabel as nib\n",
        "except ImportError:\n",
        "    nib = None\n",
        "from PIL import Image\n",
        "\n",
        "from brain_mri.utils.image_utils import ImageUtils\n",
        "from brain_mri.ml.training_utils import (\n",
        "    ExponentialMovingAverage,\n",
        "    build_densenet,\n",
        "    build_transforms,\n",
        "    focal_loss,\n",
        "    mixup_data,\n",
        "    select_device,\n",
        ")\n",
        "from brain_mri.ml.datasets import MRIDataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    precision_score,\n",
        "    r2_score,\n",
        "    recall_score,\n",
        ")\n",
        "from sklearn.svm import SVC\n",
        "import xgboost as xgb\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Evita explos\u00e3o de threads em CPUs/ARM (mesma l\u00f3gica do app original)\n",
        "import os\n",
        "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
        "\n",
        "def save_experiment(data, path=HISTORY_JSON):\n",
        "    payload = dict(data)\n",
        "    payload[\"timestamp\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    history = []\n",
        "    if path.exists():\n",
        "        try:\n",
        "            history = json.loads(path.read_text())\n",
        "        except Exception:\n",
        "            pass\n",
        "    history.append(payload)\n",
        "    path.write_text(json.dumps(history, indent=2))\n",
        "    return path\n",
        "\n",
        "def plot_confusion_matrix(ax, cm, classes, title):\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
        "                xticklabels=classes, yticklabels=classes, cbar=False)\n",
        "    ax.set_title(title, fontsize=10, fontweight='bold')\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', fontsize=8)\n",
        "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=8)\n",
        "\n",
        "def resolve_original_path(mri_id: str) -> str:\n",
        "    for ext in (\".nii.gz\", \".nii\"):\n",
        "        p = DATASET_DIR / f\"{mri_id}_axl{ext}\"\n",
        "        if p.exists():\n",
        "            try:\n",
        "                return str(p.relative_to(OUTPUT_DIR.parent)).replace(\"\\\\\", \"/\")\n",
        "            except ValueError:\n",
        "                return str(p)\n",
        "    return \"\"\n",
        "\n",
        "def calc_longitudinal(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if df.empty:\n",
        "        return df\n",
        "    if 'viable' not in df.columns:\n",
        "        df['viable'] = True\n",
        "\n",
        "    required_cols = [\n",
        "        'area_change', 'area_change_percent', 'perimeter_change', 'circularity_change',\n",
        "        'eccentricity_change', 'solidity_change', 'major_axis_change',\n",
        "        'minor_axis_change', 'visit_number'\n",
        "    ]\n",
        "    for col in required_cols:\n",
        "        if col not in df.columns:\n",
        "            df[col] = np.nan\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        m = re.search(r'MR(\\d+)', str(row.get('MRI_ID', '')))\n",
        "        if m:\n",
        "            df.at[idx, 'visit_number'] = int(m.group(1))\n",
        "\n",
        "    change_map = {\n",
        "        'ventricle_area': 'area_change',\n",
        "        'ventricle_perimeter': 'perimeter_change',\n",
        "        'ventricle_circularity': 'circularity_change',\n",
        "        'ventricle_eccentricity': 'eccentricity_change',\n",
        "        'ventricle_solidity': 'solidity_change',\n",
        "        'ventricle_major_axis_length': 'major_axis_change',\n",
        "        'ventricle_minor_axis_length': 'minor_axis_change'\n",
        "    }\n",
        "\n",
        "    if 'Subject_ID' not in df.columns:\n",
        "        return df\n",
        "\n",
        "    for sid in df['Subject_ID'].dropna().unique():\n",
        "        subj_df = df[df['Subject_ID'] == sid].sort_values('visit_number')\n",
        "        prev_idx = None\n",
        "        for idx in subj_df.index:\n",
        "            if prev_idx is None:\n",
        "                prev_idx = idx\n",
        "                continue\n",
        "            if not (bool(df.at[idx, 'viable']) and bool(df.at[prev_idx, 'viable'])):\n",
        "                prev_idx = idx\n",
        "                continue\n",
        "            for src_col, dst_col in change_map.items():\n",
        "                if src_col in df.columns:\n",
        "                    prev_val = df.at[prev_idx, src_col]\n",
        "                    cur_val = df.at[idx, src_col]\n",
        "                    if pd.notna(prev_val) and pd.notna(cur_val):\n",
        "                        df.at[idx, dst_col] = cur_val - prev_val\n",
        "            prev_area = df.at[prev_idx, 'ventricle_area'] if 'ventricle_area' in df.columns else np.nan\n",
        "            cur_area = df.at[idx, 'ventricle_area'] if 'ventricle_area' in df.columns else np.nan\n",
        "            if pd.notna(prev_area) and pd.notna(cur_area) and prev_area:\n",
        "                df.at[idx, 'area_change_percent'] = ((cur_area - prev_area) / prev_area) * 100\n",
        "            prev_idx = idx\n",
        "    return df\n",
        "\n",
        "def update_descriptors_csv(mri_id: str, descriptors: dict, seg_path: str):\n",
        "    mri_clean = mri_id.replace('_axl', '')\n",
        "    subj_id = mri_clean.split('_MR')[0] if '_MR' in mri_clean else mri_clean\n",
        "    data = {\n",
        "        'MRI_ID': mri_clean,\n",
        "        'Subject_ID': subj_id,\n",
        "        'viable': True,\n",
        "        'segmented_path': seg_path,\n",
        "        'ventricle_area': descriptors['area'],\n",
        "        'ventricle_perimeter': descriptors['perimeter'],\n",
        "        'ventricle_circularity': descriptors['circularity'],\n",
        "        'ventricle_eccentricity': descriptors['eccentricity'],\n",
        "        'ventricle_solidity': descriptors['solidity'],\n",
        "        'ventricle_major_axis_length': descriptors['major_axis_length'],\n",
        "        'ventricle_minor_axis_length': descriptors['minor_axis_length'],\n",
        "    }\n",
        "\n",
        "    df = pd.read_csv(DESCRIPTORS_CSV) if DESCRIPTORS_CSV.exists() else pd.DataFrame()\n",
        "    if df.empty and not DESCRIPTORS_CSV.exists():\n",
        "        df = pd.DataFrame(columns=data.keys())\n",
        "    if 'viable' not in df.columns:\n",
        "        df['viable'] = True\n",
        "    for key in data.keys():\n",
        "        if key not in df.columns:\n",
        "            df[key] = np.nan\n",
        "\n",
        "    if 'MRI_ID' in df.columns and mri_clean in df['MRI_ID'].values:\n",
        "        for k, v in data.items():\n",
        "            df.loc[df['MRI_ID'] == mri_clean, k] = v\n",
        "    else:\n",
        "        df = pd.concat([df, pd.DataFrame([data])], ignore_index=True)\n",
        "\n",
        "    df = calc_longitudinal(df)\n",
        "    df.to_csv(DESCRIPTORS_CSV, index=False)\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0018f3a6",
      "metadata": {},
      "source": [
        "### Segmenta\u00e7\u00e3o headless"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40f252ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Notebook auto-notes: keep split consistent; original_path rebuilt in code.\n",
        "# Fun\u00e7\u00f5es de segmenta\u00e7\u00e3o headless (carrega fatia axial, segmenta, salva PNG e descritores).\n",
        "\n",
        "def load_axial_slice(path: Path):\n",
        "    if nib is None:\n",
        "        raise ImportError(\"Instale nibabel para ler arquivos NIfTI (pip install nibabel).\")\n",
        "    data = np.squeeze(nib.load(str(path)).get_fdata())\n",
        "    if data.ndim == 3:\n",
        "        data = data[:, :, 0]\n",
        "    return ImageUtils.normalize_array(data)\n",
        "\n",
        "def segment_image(path: Path):\n",
        "    img = load_axial_slice(path)\n",
        "    mask, _, _, _ = ImageUtils.grow_region(img)\n",
        "    fname = path.stem.replace('.nii', '')\n",
        "    out_png = OUTPUT_DIR / f\"{fname}_segmented.png\"\n",
        "    Image.fromarray((mask * 255).astype(np.uint8)).save(out_png)\n",
        "    descriptors = ImageUtils.calculate_descriptors(mask)\n",
        "    update_descriptors_csv(fname, descriptors, f\"output/{out_png.name}\")\n",
        "    return {\n",
        "        'image': path,\n",
        "        'png': out_png,\n",
        "        'descriptors': descriptors,\n",
        "    }\n",
        "\n",
        "def segment_all_images(overwrite=False, limit=None, paths=None):\n",
        "    targets = sorted(paths or DATASET_DIR.glob('*.nii*'))\n",
        "    if limit:\n",
        "        targets = targets[:limit]\n",
        "    results = []\n",
        "    for i, p in enumerate(targets, 1):\n",
        "        out_png = OUTPUT_DIR / f\"{p.stem.replace('.nii', '')}_segmented.png\"\n",
        "        if out_png.exists() and not overwrite:\n",
        "            continue\n",
        "        res = segment_image(p)\n",
        "        results.append(res)\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Processados {i}/{len(targets)} arquivos\")\n",
        "    print(f\"Conclu\u00eddo: {len(results)} novas segmenta\u00e7\u00f5es salvas em {OUTPUT_DIR}\")\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6bfc0ab",
      "metadata": {},
      "source": [
        "### Criar dataset combinado (descritores + demografia)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71763877",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Notebook auto-notes: keep split consistent; original_path rebuilt in code.\n",
        "# Constr\u00f3i dataset combinado (descritores + demografia) e faz split por sujeito.\n",
        "\n",
        "def create_exam_level_dataset():\n",
        "    if not DESCRIPTORS_CSV.exists():\n",
        "        raise FileNotFoundError(\"Gere descritores primeiro (segmenta\u00e7\u00e3o).\")\n",
        "    df_desc = pd.read_csv(DESCRIPTORS_CSV)\n",
        "    if df_desc.empty:\n",
        "        raise ValueError(\"CSV de descritores est\u00e1 vazio.\")\n",
        "    if 'viable' not in df_desc.columns:\n",
        "        df_desc['viable'] = True\n",
        "\n",
        "    df_demo = pd.read_csv(CSV_DEMOGRAPHIC, sep=';', decimal=',')\n",
        "    df_demo.columns = [c.strip() for c in df_demo.columns]\n",
        "    if 'MRI ID' in df_demo.columns:\n",
        "        df_demo.rename(columns={'MRI ID': 'MRI_ID'}, inplace=True)\n",
        "    if 'Subject ID' in df_demo.columns:\n",
        "        df_demo.rename(columns={'Subject ID': 'Subject_ID'}, inplace=True)\n",
        "\n",
        "    def _as_numeric(series):\n",
        "        return pd.to_numeric(series.astype(str).str.replace(',', '.').str.strip(), errors='coerce')\n",
        "\n",
        "    numeric_map = {\n",
        "        'Age': 'age',\n",
        "        'EDUC': 'education',\n",
        "        'MMSE': 'mmse',\n",
        "        'CDR': 'cdr',\n",
        "        'eTIV': 'etiv',\n",
        "        'nWBV': 'nwbv',\n",
        "        'ASF': 'asf'\n",
        "    }\n",
        "    for src, dst in numeric_map.items():\n",
        "        if src in df_demo.columns:\n",
        "            df_demo[dst] = _as_numeric(df_demo[src])\n",
        "\n",
        "    if 'M/F' in df_demo.columns:\n",
        "        df_demo['sex'] = df_demo['M/F'].map({'M': 0, 'F': 1})\n",
        "\n",
        "    merged = pd.merge(df_desc, df_demo, on='MRI_ID', how='left', suffixes=('', '_demo'))\n",
        "    merged['viable'] = merged['viable'].fillna(True)\n",
        "    merged = merged[merged['viable'] == True]\n",
        "\n",
        "    if 'Subject_ID_x' in merged.columns:\n",
        "        merged['Subject_ID'] = merged['Subject_ID_x']\n",
        "    if 'Subject_ID_y' in merged.columns:\n",
        "        merged['Subject_ID'] = merged['Subject_ID'].fillna(merged['Subject_ID_y'])\n",
        "        merged.drop(columns=['Subject_ID_y'], inplace=True)\n",
        "    if 'Subject_ID_x' in merged.columns:\n",
        "        merged.drop(columns=['Subject_ID_x'], inplace=True)\n",
        "\n",
        "    merged['Original_Group'] = merged.get('Group')\n",
        "\n",
        "    def _resolve_final_group(row):\n",
        "        grp = row.get('Group')\n",
        "        if isinstance(grp, str) and grp == 'Converted':\n",
        "            cdr_val = row.get('cdr') if 'cdr' in row else row.get('CDR')\n",
        "            if pd.notna(cdr_val) and float(cdr_val) > 0:\n",
        "                return 'Demented'\n",
        "            return 'Nondemented'\n",
        "        return grp\n",
        "\n",
        "    merged['Final_Group'] = merged.apply(_resolve_final_group, axis=1)\n",
        "    merged['Final_Group'] = merged['Final_Group'].fillna(merged['Original_Group'])\n",
        "\n",
        "    merged['original_path'] = merged['MRI_ID'].apply(resolve_original_path)\n",
        "    merged = merged[merged['original_path'] != \"\"]\n",
        "    merged = merged[merged['Subject_ID'].notna()]\n",
        "\n",
        "    subjects = merged['Subject_ID'].unique()\n",
        "    if len(subjects) < 3:\n",
        "        raise ValueError(\"Dados insuficientes para split (m\u00ednimo 3 sujeitos).\")\n",
        "\n",
        "    train_sub, test_sub = train_test_split(subjects, test_size=0.2)\n",
        "    train_sub, val_sub = train_test_split(train_sub, test_size=0.2)\n",
        "\n",
        "    def get_split(sid):\n",
        "        if sid in val_sub: return 'validation'\n",
        "        if sid in test_sub: return 'test'\n",
        "        return 'train'\n",
        "\n",
        "    merged['split'] = merged['Subject_ID'].apply(get_split)\n",
        "\n",
        "    cols_to_drop = ['Age', 'EDUC', 'SES', 'MMSE', 'CDR', 'eTIV', 'nWBV', 'ASF', 'Visit', 'MR Delay', 'M/F']\n",
        "    cols_to_drop = [c for c in cols_to_drop if c in merged.columns]\n",
        "    if cols_to_drop:\n",
        "        merged.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "    merged.to_csv(EXAM_SPLIT_CSV, index=False)\n",
        "    print(f\"Dataset salvo em {EXAM_SPLIT_CSV} | Total: {len(merged)} exames\")\n",
        "    return merged\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d6ebc58",
      "metadata": {},
      "source": [
        "### Treinos cl\u00e1ssicos: SVM (classifica\u00e7\u00e3o) e XGBoost (regress\u00e3o de idade)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5516f6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Notebook auto-notes: keep split consistent; original_path rebuilt in code.\n",
        "# Treinos cl\u00e1ssicos (SVM classifica\u00e7\u00e3o e XGBoost regress\u00e3o) com grids e salvamento de m\u00e9tricas.\n",
        "\n",
        "DEFAULT_SVM_FEATURES = [\n",
        "    'ventricle_area', 'ventricle_perimeter', 'ventricle_circularity',\n",
        "    'ventricle_eccentricity', 'mmse', 'cdr', 'age'\n",
        "]\n",
        "\n",
        "DEFAULT_XGB_FEATURES = [\n",
        "    'ventricle_area', 'ventricle_perimeter', 'ventricle_circularity',\n",
        "    'ventricle_eccentricity', 'mmse', 'cdr', 'nwbv', 'etiv', 'asf', 'sex', 'education'\n",
        "]\n",
        "\n",
        "def train_svm_classifier(features=None):\n",
        "    if not EXAM_SPLIT_CSV.exists():\n",
        "        raise FileNotFoundError(\"Crie o dataset combinado primeiro.\")\n",
        "    df = pd.read_csv(EXAM_SPLIT_CSV)\n",
        "    features = features or DEFAULT_SVM_FEATURES\n",
        "\n",
        "    tmp = df.copy()\n",
        "    if 'sex' in features and 'sex' not in tmp.columns:\n",
        "        if 'M/F' in tmp.columns:\n",
        "            tmp['sex'] = tmp['M/F'].map({'M': 0, 'F': 1})\n",
        "        else:\n",
        "            tmp['sex'] = np.nan\n",
        "\n",
        "    missing = [f for f in features if f not in tmp.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Colunas ausentes no dataset: {missing}\")\n",
        "\n",
        "    X = tmp[features].copy().fillna(tmp[features].mean()).values\n",
        "    y = (tmp['Final_Group'] == 'Demented').astype(int).values\n",
        "\n",
        "    train_mask = df['split'] == 'train'\n",
        "    val_mask = df['split'] == 'validation'\n",
        "    test_mask = df['split'] == 'test'\n",
        "    if not val_mask.any():\n",
        "        raise ValueError(\"Split de valida\u00e7\u00e3o vazio.\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X[train_mask])\n",
        "    X_val = scaler.transform(X[val_mask])\n",
        "    X_test = scaler.transform(X[test_mask]) if test_mask.any() else None\n",
        "\n",
        "    grid = {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
        "        'kernel': ['rbf', 'linear']\n",
        "    }\n",
        "    gs = GridSearchCV(SVC(), grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "    gs.fit(X_train, y[train_mask])\n",
        "    clf = gs.best_estimator_\n",
        "\n",
        "    y_train_pred = clf.predict(X_train)\n",
        "    y_val_pred = clf.predict(X_val)\n",
        "    acc_tr = accuracy_score(y[train_mask], y_train_pred)\n",
        "    acc_val = accuracy_score(y[val_mask], y_val_pred)\n",
        "\n",
        "    test_cm = None\n",
        "    metrics = {\n",
        "        'train_accuracy': float(acc_tr),\n",
        "        'val_accuracy': float(acc_val),\n",
        "        'best_params': gs.best_params_,\n",
        "    }\n",
        "    if X_test is not None:\n",
        "        y_test_pred = clf.predict(X_test)\n",
        "        acc_test = accuracy_score(y[test_mask], y_test_pred)\n",
        "        test_cm = confusion_matrix(y[test_mask], y_test_pred)\n",
        "        metrics.update({\n",
        "            'test_accuracy': float(acc_test),\n",
        "            'test_precision': float(precision_score(y[test_mask], y_test_pred, zero_division=0)),\n",
        "            'test_recall': float(recall_score(y[test_mask], y_test_pred, zero_division=0)),\n",
        "            'test_f1': float(f1_score(y[test_mask], y_test_pred, zero_division=0)),\n",
        "        })\n",
        "\n",
        "    if test_cm is not None:\n",
        "        fig_cm, ax = plt.subplots(figsize=(6, 5))\n",
        "        plot_confusion_matrix(ax, test_cm, ['0', '1'], \"Teste\")\n",
        "        fig_cm.tight_layout()\n",
        "        cm_path = OUTPUT_DIR / \"confusion_svm.png\"\n",
        "        fig_cm.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close(fig_cm)\n",
        "        metrics['confusion_matrix'] = cm_path.name\n",
        "\n",
        "    import pickle\n",
        "    with open(OUTPUT_DIR / \"svm_scaler.pkl\", \"wb\") as f:\n",
        "        pickle.dump(scaler, f)\n",
        "    with open(OUTPUT_DIR / \"svm_model.pkl\", \"wb\") as f:\n",
        "        pickle.dump(clf, f)\n",
        "\n",
        "    metrics['model'] = 'SVM'\n",
        "    metrics['features'] = features\n",
        "    save_experiment(metrics)\n",
        "    print(\"SVM treinado.\", metrics)\n",
        "    return metrics\n",
        "\n",
        "def train_xgboost_regressor(features=None):\n",
        "    if not EXAM_SPLIT_CSV.exists():\n",
        "        raise FileNotFoundError(\"Crie o dataset combinado primeiro.\")\n",
        "    df = pd.read_csv(EXAM_SPLIT_CSV)\n",
        "    features = features or DEFAULT_XGB_FEATURES\n",
        "\n",
        "    tmp = df.copy()\n",
        "    if 'sex' in features and 'sex' not in tmp.columns and 'M/F' in tmp.columns:\n",
        "        tmp['sex'] = tmp['M/F'].map({'M': 0, 'F': 1})\n",
        "\n",
        "    missing = [f for f in features if f not in tmp.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Colunas ausentes no dataset: {missing}\")\n",
        "\n",
        "    X = tmp[features].fillna(tmp[features].mean()).values\n",
        "    y = tmp['age'].values\n",
        "\n",
        "    train_mask = df['split'] == 'train'\n",
        "    val_mask = df['split'] == 'validation'\n",
        "    if not val_mask.any():\n",
        "        raise ValueError(\"Split de valida\u00e7\u00e3o vazio.\")\n",
        "\n",
        "    groups = df.loc[train_mask, 'Subject_ID']\n",
        "    base = xgb.XGBRegressor(objective='reg:squarederror', tree_method='hist', n_jobs=1, verbosity=0)\n",
        "    grid = {\n",
        "        'n_estimators': [200, 300, 500],\n",
        "        'max_depth': [6, 8, 10],\n",
        "        'learning_rate': [0.05, 0.1, 0.15],\n",
        "        'min_child_weight': [1, 3, 5],\n",
        "        'subsample': [0.8, 0.9],\n",
        "        'colsample_bytree': [0.8, 0.9]\n",
        "    }\n",
        "\n",
        "    gkf = GroupKFold(n_splits=3)\n",
        "    gs = GridSearchCV(base, grid, cv=gkf.split(X[train_mask], y[train_mask], groups),\n",
        "                      scoring='neg_mean_absolute_error', n_jobs=-1, verbose=1)\n",
        "    gs.fit(X[train_mask], y[train_mask])\n",
        "    model = gs.best_estimator_\n",
        "\n",
        "    val_preds = model.predict(X[val_mask])\n",
        "    mae_val = mean_absolute_error(y[val_mask], val_preds)\n",
        "    mse_val = mean_squared_error(y[val_mask], val_preds)\n",
        "    rmse_val = float(np.sqrt(mse_val))\n",
        "    r2_val = r2_score(y[val_mask], val_preds)\n",
        "\n",
        "    import pickle\n",
        "    with open(OUTPUT_DIR / \"xgb_age.pkl\", \"wb\") as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    metrics = {\n",
        "        'model': 'XGBoost',\n",
        "        'target': 'age',\n",
        "        'features': features,\n",
        "        'val_mae': float(mae_val),\n",
        "        'val_mse': float(mse_val),\n",
        "        'val_rmse': float(rmse_val),\n",
        "        'val_r2': float(r2_val),\n",
        "        'best_params': gs.best_params_,\n",
        "    }\n",
        "    save_experiment(metrics)\n",
        "    print(\"XGBoost treinado.\", metrics)\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f6509d6",
      "metadata": {},
      "source": [
        "### Treino DenseNet (classifica\u00e7\u00e3o ou regress\u00e3o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1111208",
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    if hasattr(torch.backends, \"cudnn\"):\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "# Notebook auto-notes: keep split consistent; original_path rebuilt in code.\n",
        "# Treino DenseNet (classifica\u00e7\u00e3o/regress\u00e3o) com mixup/focal/EMA, curvas, m\u00e9tricas e export de embeddings.\n",
        "import time\n",
        "\n",
        "def train_densenet(mode='classification', hparams=None, export_embeddings=True, max_epochs=None):\n",
        "    set_seed(42)\n",
        "    if not EXAM_SPLIT_CSV.exists():\n",
        "        raise FileNotFoundError(\"Crie o dataset combinado primeiro.\")\n",
        "    df = pd.read_csv(EXAM_SPLIT_CSV)\n",
        "    device = select_device()\n",
        "    print(f\"Dispositivo: {device} | Torch threads: {torch.get_num_threads()}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    defaults = {\n",
        "        \"lr\": 1e-4 if mode == 'classification' else 0.001,\n",
        "        \"weight_decay\": 1e-4 if mode == 'classification' else 0.0,\n",
        "        \"dropout\": 0.3,\n",
        "        \"label_smoothing\": 0.05 if mode == 'classification' else 0.0,\n",
        "        \"mixup_alpha\": 0.4 if mode == 'classification' else 0.0,\n",
        "        \"freeze_backbone\": False,\n",
        "        \"freeze_warmup_epochs\": 0,\n",
        "        \"warmup_lr\": None,\n",
        "        \"class_balance\": False,\n",
        "        \"balance_penalty\": 0.25,\n",
        "        \"thresholds_eval\": [0.5, 0.6, 0.4, 0.7],\n",
        "    }\n",
        "    if hparams:\n",
        "        for k, v in hparams.items():\n",
        "            if k in defaults:\n",
        "                defaults[k] = v\n",
        "\n",
        "    lr = defaults[\"lr\"]\n",
        "    weight_decay = defaults[\"weight_decay\"]\n",
        "    dropout_rate = defaults[\"dropout\"]\n",
        "    label_smoothing = defaults[\"label_smoothing\"]\n",
        "    mixup_alpha = defaults[\"mixup_alpha\"]\n",
        "    freeze_backbone = bool(defaults[\"freeze_backbone\"])\n",
        "    freeze_warmup_epochs = int(defaults.get(\"freeze_warmup_epochs\", 0) or 0)\n",
        "    warmup_lr = defaults.get(\"warmup_lr\", None)\n",
        "    use_class_balance = bool(defaults[\"class_balance\"])\n",
        "    balance_penalty = defaults.get(\"balance_penalty\", 0.0)\n",
        "    thresholds_eval = defaults.get(\"thresholds_eval\", [0.5])\n",
        "\n",
        "    age_scaler = None\n",
        "    if mode == 'regression':\n",
        "        age_scaler = StandardScaler()\n",
        "        df_train = df[df['split']=='train'].copy()\n",
        "        df_val = df[df['split']=='validation'].copy()\n",
        "        df_test = df[df['split']=='test'].copy()\n",
        "        df_train['age_normalized'] = age_scaler.fit_transform(df_train[['age']])\n",
        "        df_val['age_normalized'] = age_scaler.transform(df_val[['age']])\n",
        "        df_test['age_normalized'] = age_scaler.transform(df_test[['age']])\n",
        "        df.loc[df['split']=='train', 'age_normalized'] = df_train['age_normalized']\n",
        "        df.loc[df['split']=='validation', 'age_normalized'] = df_val['age_normalized']\n",
        "        df.loc[df['split']=='test', 'age_normalized'] = df_test['age_normalized']\n",
        "        print(f\"Idade normalizada: [{df_train['age_normalized'].min():.2f}, {df_train['age_normalized'].max():.2f}]\")\n",
        "\n",
        "    train_tf, val_tf = build_transforms()\n",
        "    lbl_col = 'age_normalized' if mode == 'regression' else 'Final_Group'\n",
        "    train_ds = MRIDataset(df[df['split']=='train'], train_tf, DATASET_DIR.parent, 'original_path', lbl_col)\n",
        "    val_ds = MRIDataset(df[df['split']=='validation'], val_tf, DATASET_DIR.parent, 'original_path', lbl_col)\n",
        "    test_ds = MRIDataset(df[df['split']=='test'], val_tf, DATASET_DIR.parent, 'original_path', lbl_col)\n",
        "    if len(val_ds) == 0:\n",
        "        raise ValueError(\"Split de valida\u00e7\u00e3o vazio.\")\n",
        "\n",
        "    epochs = max_epochs or (40 if mode == 'classification' else 20)\n",
        "    batch_size = 16\n",
        "    early_stop_patience = 7 if mode == 'classification' else None\n",
        "    use_mixup = mode == 'classification' and mixup_alpha > 0\n",
        "    use_focal = mode == 'classification' and not use_mixup\n",
        "    focal_gamma = 1.5\n",
        "    use_ema = mode == 'classification'\n",
        "    ema_decay = 0.999\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
        "\n",
        "    model = build_densenet(mode=mode, dropout_rate=dropout_rate).to(device)\n",
        "    warmup_epochs_remaining = freeze_warmup_epochs\n",
        "    if warmup_epochs_remaining > 0 and hasattr(model, \"features\"):\n",
        "        for p in model.features.parameters():\n",
        "            p.requires_grad = False\n",
        "        current_lr = warmup_lr if warmup_lr is not None else lr * 0.5\n",
        "    else:\n",
        "        current_lr = lr\n",
        "    if freeze_backbone and warmup_epochs_remaining == 0 and hasattr(model, \"features\"):\n",
        "        for p in model.features.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=current_lr, weight_decay=weight_decay)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=lr * 0.1)\n",
        "    class_weights = None\n",
        "    if mode == 'classification' and use_class_balance and 'Final_Group' in df.columns:\n",
        "        counts = df[df['split'] == 'train']['Final_Group'].value_counts()\n",
        "        if len(counts) >= 1:\n",
        "            total = counts.sum()\n",
        "            w0 = total / (2 * counts.get('Nondemented', max(counts.max(), 1)))\n",
        "            w1 = total / (2 * counts.get('Demented', max(counts.max(), 1)))\n",
        "            class_weights = torch.tensor([w0, w1], dtype=torch.float32, device=device)\n",
        "\n",
        "    criterion = nn.MSELoss() if mode == 'regression' else nn.CrossEntropyLoss(\n",
        "        label_smoothing=label_smoothing,\n",
        "        weight=class_weights\n",
        "    )\n",
        "    ema = ExponentialMovingAverage(model, decay=ema_decay) if use_ema else None\n",
        "\n",
        "    history_train_loss, history_val_loss = [], []\n",
        "    history_train_acc, history_val_acc = [], []\n",
        "    history_train_mae, history_val_mae = [], []\n",
        "    best_state, best_epoch = None, 0\n",
        "    best_val_metric = -float('inf') if mode == 'classification' else float('inf')\n",
        "    no_improve = 0\n",
        "    val_metric_value = None\n",
        "    test_cm = None\n",
        "    test_cm_list = None\n",
        "    test_acc = test_precision = test_recall = test_f1 = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        total_train = 0\n",
        "        correct_train, total_train_cls = 0, 0\n",
        "        mae_sum_train, total_train_reg = 0.0, 0\n",
        "\n",
        "        for imgs, lbls in train_loader:\n",
        "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            if mode == 'regression':\n",
        "                out = model(imgs)\n",
        "                preds_batch = out.squeeze()\n",
        "                loss = criterion(preds_batch, lbls)\n",
        "                mae_sum_train += torch.abs(preds_batch - lbls).sum().item()\n",
        "                total_train_reg += lbls.size(0)\n",
        "            else:\n",
        "                if use_mixup:\n",
        "                    imgs_mix, targets_a, targets_b, lam = mixup_data(imgs, lbls.long(), mixup_alpha)\n",
        "                    out = model(imgs_mix)\n",
        "                    loss = lam * criterion(out, targets_a) + (1 - lam) * criterion(out, targets_b)\n",
        "                    preds_batch = out.argmax(dim=1)\n",
        "                    if balance_penalty > 0:\n",
        "                        p_mean = torch.softmax(out, dim=1)[:, 1].mean()\n",
        "                        loss = loss + balance_penalty * torch.abs(p_mean - 0.5)\n",
        "                    correct_train += (\n",
        "                        lam * (preds_batch == targets_a).sum().item()\n",
        "                        + (1 - lam) * (preds_batch == targets_b).sum().item()\n",
        "                    )\n",
        "                else:\n",
        "                    out = model(imgs)\n",
        "                    loss = focal_loss(out, lbls.long(), gamma=focal_gamma) if use_focal else criterion(out, lbls.long())\n",
        "                    preds_batch = out.argmax(dim=1)\n",
        "                    correct_train += (preds_batch == lbls.long()).sum().item()\n",
        "                total_train_cls += lbls.size(0)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if ema: ema.update(model)\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "            total_train += imgs.size(0)\n",
        "\n",
        "        model.eval()\n",
        "        running_val = 0\n",
        "        preds_list, targs_list = [], []\n",
        "        correct_val, total_val = 0, 0\n",
        "        if ema: ema.apply_shadow(model)\n",
        "        with torch.no_grad():\n",
        "            for imgs, lbls in val_loader:\n",
        "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
        "                out = model(imgs)\n",
        "                if mode == 'regression':\n",
        "                    loss = criterion(out.squeeze(), lbls)\n",
        "                    running_val += loss.item() * imgs.size(0)\n",
        "                    preds_list.append(out.squeeze().cpu().numpy())\n",
        "                    targs_list.append(lbls.cpu().numpy())\n",
        "                else:\n",
        "                    loss = focal_loss(out, lbls.long(), gamma=focal_gamma) if use_focal else criterion(out, lbls.long())\n",
        "                    running_val += loss.item() * imgs.size(0)\n",
        "                    preds = out.argmax(dim=1)\n",
        "                    correct_val += (preds == lbls.long()).sum().item()\n",
        "                    total_val += lbls.size(0)\n",
        "        if ema: ema.restore(model)\n",
        "\n",
        "        train_loss = running_loss / max(total_train, 1)\n",
        "        val_loss = running_val / max(len(val_ds), 1)\n",
        "        history_train_loss.append(train_loss)\n",
        "        history_val_loss.append(val_loss)\n",
        "\n",
        "        if mode == 'regression':\n",
        "            train_mae = mae_sum_train / max(total_train_reg, 1)\n",
        "            history_train_mae.append(train_mae)\n",
        "            if preds_list:\n",
        "                preds = np.concatenate(preds_list)\n",
        "                targets = np.concatenate(targs_list)\n",
        "                val_metric_value = mean_absolute_error(targets, preds)\n",
        "                history_val_mae.append(val_metric_value)\n",
        "        else:\n",
        "            train_acc = correct_train / max(total_train_cls, 1) if total_train_cls else 0.0\n",
        "            history_train_acc.append(train_acc)\n",
        "            if total_val:\n",
        "                val_metric_value = correct_val / total_val\n",
        "                history_val_acc.append(val_metric_value)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}: Train Loss {train_loss:.4f}, Val Loss {val_loss:.4f}\")\n",
        "\n",
        "        improved = False\n",
        "        if mode == 'classification' and val_metric_value is not None:\n",
        "            if val_metric_value > best_val_metric:\n",
        "                improved = True\n",
        "                best_val_metric = val_metric_value\n",
        "        elif mode == 'regression' and val_loss < best_val_metric:\n",
        "            improved = True\n",
        "            best_val_metric = val_loss\n",
        "\n",
        "        if improved:\n",
        "            best_epoch = epoch + 1\n",
        "            no_improve = 0\n",
        "            best_state = {k: v.cpu() for k, v in (ema.shadow if (ema and ema.shadow) else model.state_dict()).items()}\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if early_stop_patience and no_improve >= early_stop_patience:\n",
        "                print(f\"Early stopping na \u00e9poca {epoch+1}. Melhor \u00e9poca: {best_epoch}\")\n",
        "                break\n",
        "        scheduler.step()\n",
        "\n",
        "        model.load_state_dict(best_state, strict=False)\n",
        "\n",
        "    history_train_mae_denorm = history_train_mae\n",
        "    history_val_mae_denorm = history_val_mae\n",
        "    if mode == 'regression' and age_scaler is not None:\n",
        "        mae_scale = age_scaler.scale_[0]\n",
        "        history_train_mae_denorm = [mae * mae_scale for mae in history_train_mae]\n",
        "        history_val_mae_denorm = [mae * mae_scale for mae in history_val_mae]\n",
        "        print(f\"MAE (orig): train={history_train_mae_denorm[-1]:.4f} | val={history_val_mae_denorm[-1]:.4f}\")\n",
        "\n",
        "    if history_train_loss:\n",
        "        epochs_range = range(1, len(history_train_loss) + 1)\n",
        "        fig = plt.figure(figsize=(10, 4))\n",
        "        ax1 = fig.add_subplot(121)\n",
        "        ax1.plot(epochs_range, history_train_loss, 'b-', label='Treino')\n",
        "        ax1.plot(epochs_range, history_val_loss, 'r-', label='Valida\u00e7\u00e3o')\n",
        "        ax1.set_title(\"Loss\")\n",
        "        ax1.set_xlabel(\"\u00c9poca\")\n",
        "        ax1.legend(); ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        ax2 = fig.add_subplot(122)\n",
        "        if mode == 'classification':\n",
        "            if history_train_acc: ax2.plot(epochs_range, history_train_acc, 'b-', label='Treino')\n",
        "            if history_val_acc: ax2.plot(epochs_range, history_val_acc, 'r-', label='Valida\u00e7\u00e3o')\n",
        "            ax2.set_title(\"Acur\u00e1cia\")\n",
        "        else:\n",
        "            if history_train_mae_denorm: ax2.plot(epochs_range, history_train_mae_denorm, 'b-', label='Treino')\n",
        "            if history_val_mae_denorm: ax2.plot(epochs_range, history_val_mae_denorm, 'r-', label='Valida\u00e7\u00e3o')\n",
        "            ax2.set_title(\"MAE (anos)\")\n",
        "        ax2.set_xlabel(\"\u00c9poca\")\n",
        "        ax2.legend(); ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        curves_name = f\"densenet_{mode}_learning_curves.png\"\n",
        "        fig.tight_layout()\n",
        "        fig.savefig(OUTPUT_DIR / curves_name, dpi=300, bbox_inches='tight')\n",
        "        plt.close(fig)\n",
        "\n",
        "    torch.save(model.state_dict(), OUTPUT_DIR / f\"densenet_{mode}.pth\")\n",
        "    if best_state is not None:\n",
        "        torch.save(best_state, OUTPUT_DIR / f\"densenet_{mode}_bestval.pth\")\n",
        "\n",
        "    val_metric_value = None\n",
        "    test_cm = None\n",
        "    train_mae_orig = val_mae_orig = test_mae_orig = None\n",
        "    train_r2 = val_r2 = test_r2 = train_rmse = val_rmse = test_rmse = None\n",
        "\n",
        "    if mode == 'regression' and age_scaler is not None:\n",
        "        model.eval()\n",
        "        all_preds = {'train': [], 'val': [], 'test': []}\n",
        "        all_true = {'train': [], 'val': [], 'test': []}\n",
        "        loaders = {'train': train_loader, 'val': val_loader, 'test': test_loader}\n",
        "        with torch.no_grad():\n",
        "            for split, loader in loaders.items():\n",
        "                for imgs, ages in loader:\n",
        "                    imgs = imgs.to(device)\n",
        "                    preds = model(imgs).squeeze()\n",
        "                    all_preds[split].extend(np.atleast_1d(preds.cpu().numpy()))\n",
        "                    all_true[split].extend(np.atleast_1d(ages.numpy()))\n",
        "\n",
        "        for k in all_preds:\n",
        "            all_preds[k] = age_scaler.inverse_transform(np.array(all_preds[k]).reshape(-1, 1)).flatten()\n",
        "            all_true[k] = age_scaler.inverse_transform(np.array(all_true[k]).reshape(-1, 1)).flatten()\n",
        "\n",
        "        train_mae_orig = mean_absolute_error(all_true['train'], all_preds['train'])\n",
        "        train_r2 = r2_score(all_true['train'], all_preds['train'])\n",
        "        train_rmse = np.sqrt(mean_squared_error(all_true['train'], all_preds['train']))\n",
        "        val_mae_orig = mean_absolute_error(all_true['val'], all_preds['val'])\n",
        "        val_r2 = r2_score(all_true['val'], all_preds['val'])\n",
        "        val_rmse = np.sqrt(mean_squared_error(all_true['val'], all_preds['val']))\n",
        "        test_mae_orig = mean_absolute_error(all_true['test'], all_preds['test'])\n",
        "        test_r2 = r2_score(all_true['test'], all_preds['test'])\n",
        "        test_rmse = np.sqrt(mean_squared_error(all_true['test'], all_preds['test']))\n",
        "        val_metric_value = test_mae_orig\n",
        "\n",
        "        fig_scatter = plt.figure(figsize=(8, 7))\n",
        "        ax = fig_scatter.add_subplot(111)\n",
        "        ax.scatter(all_true['test'], all_preds['test'], alpha=0.6, s=80, c='green',\n",
        "                   edgecolors='darkgreen', linewidths=0.5)\n",
        "        min_val = min(all_true['test'].min(), all_preds['test'].min())\n",
        "        max_val = max(all_true['test'].max(), all_preds['test'].max())\n",
        "        ax.text(0.05, 0.95,\n",
        "                f'R\u00b2 = {test_r2:.4f}\\n'\n",
        "                f'MAE = {test_mae_orig:.2f} anos\\n'\n",
        "                f'RMSE = {test_rmse:.2f} anos\\n'\n",
        "                f'N = {len(all_true['test'])} amostras',\n",
        "                transform=ax.transAxes, fontsize=12, verticalalignment='top',\n",
        "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.9,\n",
        "                         edgecolor='darkgreen', linewidth=2))\n",
        "        ax.set_xlabel('Idade Real (anos)', fontsize=13, fontweight='bold')\n",
        "        ax.set_ylabel('Idade Predita (anos)', fontsize=13, fontweight='bold')\n",
        "        ax.set_title('Teste: Predito vs Real', fontsize=14, fontweight='bold', pad=15)\n",
        "        ax.legend(loc='lower right', fontsize=11, framealpha=0.9)\n",
        "        ax.grid(True, alpha=0.3, linestyle='--')\n",
        "    if mode == 'classification':\n",
        "        model.eval()\n",
        "        test_cm_list = None\n",
        "        y_true_test, y_pred_test = [], []\n",
        "        with torch.no_grad():\n",
        "            for imgs, lbls in test_loader:\n",
        "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
        "                out = model(imgs)\n",
        "                preds = out.argmax(dim=1)\n",
        "                y_true_test.append(lbls.cpu().numpy())\n",
        "                y_pred_test.append(preds.cpu().numpy())\n",
        "        if y_true_test:\n",
        "            y_true_test = np.concatenate(y_true_test)\n",
        "            y_pred_test = np.concatenate(y_pred_test)\n",
        "            test_cm = confusion_matrix(y_true_test, y_pred_test)\n",
        "            test_acc = accuracy_score(y_true_test, y_pred_test)\n",
        "            test_cm_list = test_cm.tolist()\n",
        "            test_precision = precision_score(y_true_test, y_pred_test, average='binary', zero_division=0)\n",
        "            test_recall = recall_score(y_true_test, y_pred_test, average='binary', zero_division=0)\n",
        "            test_f1 = f1_score(y_true_test, y_pred_test, average='binary', zero_division=0)\n",
        "            val_metric_value = test_acc\n",
        "\n",
        "            fig_cm, ax = plt.subplots(figsize=(6,5))\n",
        "            plot_confusion_matrix(ax, test_cm, ['Nondemented', 'Demented'], \"Teste\")\n",
        "            fig_cm.tight_layout()\n",
        "            fig_cm.savefig(OUTPUT_DIR / f\"confusion_densenet_{mode}.png\", dpi=300, bbox_inches='tight')\n",
        "            plt.close(fig_cm)\n",
        "\n",
        "    learning_curves = {\n",
        "        'train_loss': history_train_loss,\n",
        "        'val_loss': history_val_loss,\n",
        "    }\n",
        "    if mode == 'classification':\n",
        "        learning_curves['train_acc'] = history_train_acc\n",
        "        learning_curves['val_acc'] = history_val_acc\n",
        "    else:\n",
        "        learning_curves['train_mae'] = history_train_mae_denorm\n",
        "        learning_curves['val_mae'] = history_val_mae_denorm\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    exp_payload = {\n",
        "        'model': f'DenseNet_{mode}',\n",
        "        'epochs': len(history_train_loss),\n",
        "        'batch_size': batch_size,\n",
        "        'learning_rate': lr,\n",
        "        'train_loss': float(history_train_loss[-1]) if history_train_loss else None,\n",
        "        'val_loss': float(history_val_loss[-1]) if history_val_loss else None,\n",
        "        'learning_curves': learning_curves,\n",
        "        'training_time_seconds': float(training_time),\n",
        "        'best_params': {\n",
        "            'epochs': epochs,\n",
        "            'batch_size': batch_size,\n",
        "            'learning_rate': lr,\n",
        "        }\n",
        "    }\n",
        "    if mode == 'classification':\n",
        "        exp_payload['best_val_accuracy'] = float(best_val_metric) if best_val_metric != -float('inf') else None\n",
        "        exp_payload['best_epoch'] = best_epoch\n",
        "        if val_metric_value is not None:\n",
        "            exp_payload['test_accuracy'] = float(val_metric_value)\n",
        "        if test_precision is not None:\n",
        "            exp_payload['test_precision'] = float(test_precision)\n",
        "            exp_payload['test_recall'] = float(test_recall)\n",
        "            exp_payload['test_f1'] = float(test_f1)\n",
        "        if test_cm_list is not None:\n",
        "            exp_payload['test_confusion_matrix'] = test_cm_list\n",
        "            exp_payload['test_classes'] = ['Nondemented', 'Demented']\n",
        "        if 'best_thr' in locals() and best_thr:\n",
        "            exp_payload['best_threshold'] = best_thr\n",
        "            exp_payload['threshold_candidates'] = threshold_metrics\n",
        "    else:\n",
        "        exp_payload.update({\n",
        "            'type': 'regression',\n",
        "            'test_mae': float(test_mae_orig) if test_mae_orig is not None else None,\n",
        "            'train_mae': float(train_mae_orig) if train_mae_orig is not None else None,\n",
        "            'val_mae': float(val_mae_orig) if val_mae_orig is not None else None,\n",
        "            'train_r2': float(train_r2) if train_r2 is not None else None,\n",
        "            'val_r2': float(val_r2) if val_r2 is not None else None,\n",
        "            'test_r2': float(test_r2) if test_r2 is not None else None,\n",
        "            'train_rmse': float(train_rmse) if train_rmse is not None else None,\n",
        "            'val_rmse': float(val_rmse) if val_rmse is not None else None,\n",
        "            'test_rmse': float(test_rmse) if test_rmse is not None else None,\n",
        "        })\n",
        "\n",
        "    if export_embeddings:\n",
        "        try:\n",
        "            def _export_embeddings(split_name, dataset_obj):\n",
        "                if len(dataset_obj) == 0:\n",
        "                    return\n",
        "                loader = DataLoader(dataset_obj, batch_size=batch_size, shuffle=False)\n",
        "                emb_list, target_list, ids = [], [], []\n",
        "                model.eval()\n",
        "                idx_offset = 0\n",
        "                with torch.no_grad():\n",
        "                    for imgs, lbls in loader:\n",
        "                        imgs = imgs.to(device)\n",
        "                        feats = model.features(imgs)\n",
        "                        feats = F.relu(feats, inplace=False)\n",
        "                        feats = F.adaptive_avg_pool2d(feats, (1, 1)).view(feats.size(0), -1)\n",
        "                        emb_list.append(feats.cpu().numpy())\n",
        "                        target_list.append(lbls.cpu().numpy())\n",
        "                        rows = dataset_obj.df.iloc[idx_offset: idx_offset + len(lbls)]\n",
        "                        ids.extend(rows.get('MRI_ID', rows.index).tolist())\n",
        "                        idx_offset += len(lbls)\n",
        "                emb_arr = np.concatenate(emb_list)\n",
        "                tgt_arr = np.concatenate(target_list)\n",
        "                df_emb = pd.DataFrame(emb_arr)\n",
        "                df_emb.insert(0, 'MRI_ID', ids)\n",
        "                if mode == 'regression' and 'age' in dataset_obj.df.columns:\n",
        "                    df_emb['target'] = dataset_obj.df.loc[:len(df_emb)-1, 'age'].values\n",
        "                else:\n",
        "                    df_emb['target'] = tgt_arr\n",
        "                out_path = OUTPUT_DIR / f\"densenet_embeddings_{mode}_{split_name}.csv\"\n",
        "                df_emb.to_csv(out_path, index=False)\n",
        "            _export_embeddings('train', train_ds)\n",
        "            _export_embeddings('val', val_ds)\n",
        "            _export_embeddings('test', test_ds)\n",
        "        except Exception as e:\n",
        "            print(f\"Falha ao exportar embeddings: {e}\")\n",
        "\n",
        "    save_experiment(exp_payload)\n",
        "    print(\"Treino DenseNet finalizado.\", exp_payload)\n",
        "    return exp_payload\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "752cf3b0",
      "metadata": {},
      "source": [
        "### Refinamento com RL (DenseNet)\n",
        "\n",
        "Porta a l\u00f3gica de `refine_densenet_with_rl` para rodar sem Tkinter: define ambiente PPO simplificado, treina em micro-\u00e9pocas sobre um subset e salva o melhor checkpoint (densenet_classification_rl_best.pth) + pol\u00edtica (rl_policy_densenet.pth) e hist\u00f3rico JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "465a0701",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Refinamento por RL da DenseNet: PPO simplificado, ambiente de micro-treinos e treino final com melhores hiperpar\u00e2metros.\n",
        "\n",
        "import math\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "class TrainHistoryWriter:\n",
        "    def __init__(self, out_dir):\n",
        "        self.out_dir = Path(out_dir)\n",
        "        self.out_dir.mkdir(exist_ok=True)\n",
        "    def save(self, history):\n",
        "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        path = self.out_dir / f\"train_history_rl_{ts}.json\"\n",
        "        path.write_text(json.dumps(history, indent=2))\n",
        "        return path\n",
        "\n",
        "class ActorCritic(torch.nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.shared = torch.nn.Sequential(\n",
        "            torch.nn.Linear(state_dim, 64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(64, 64),\n",
        "            torch.nn.ReLU(),\n",
        "        )\n",
        "        self.policy_head = torch.nn.Linear(64, action_dim)\n",
        "        self.value_head = torch.nn.Linear(64, 1)\n",
        "    def forward(self, x):\n",
        "        x = self.shared(x)\n",
        "        return self.policy_head(x), self.value_head(x)\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self, state_dim, action_dim, device, lr=3e-4, gamma=0.99, clip_eps=0.2, epochs=4, batch_size=64):\n",
        "        self.device = device\n",
        "        self.gamma = gamma\n",
        "        self.clip_eps = clip_eps\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.policy = ActorCritic(state_dim, action_dim).to(device)\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.memory = []\n",
        "    def select_action(self, state):\n",
        "        state_t = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "        logits, value = self.policy(state_t)\n",
        "        dist = Categorical(logits=logits)\n",
        "        action = dist.sample()\n",
        "        return int(action.item()), dist.log_prob(action), value.squeeze(0)\n",
        "    def store(self, state, action, log_prob, value, reward, done):\n",
        "        self.memory.append({\n",
        "            'state': torch.tensor(state, dtype=torch.float32),\n",
        "            'action': torch.tensor(action),\n",
        "            'log_prob': log_prob.detach(),\n",
        "            'value': value.detach(),\n",
        "            'reward': torch.tensor(reward, dtype=torch.float32),\n",
        "            'done': torch.tensor(done, dtype=torch.float32),\n",
        "        })\n",
        "    def _compute_returns_adv(self, gamma):\n",
        "        returns, advs = [], []\n",
        "        R = 0\n",
        "        for step in reversed(self.memory):\n",
        "            R = step['reward'] + gamma * R * (1 - step['done'])\n",
        "            returns.insert(0, R)\n",
        "        returns = torch.stack(returns)\n",
        "        values = torch.stack([m['value'] for m in self.memory]).squeeze(-1)\n",
        "        advs = returns - values\n",
        "        advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n",
        "        return returns.detach(), advs.detach()\n",
        "    def update(self):\n",
        "        if not self.memory:\n",
        "            return {}\n",
        "        states = torch.stack([m['state'] for m in self.memory]).to(self.device)\n",
        "        actions = torch.stack([m['action'] for m in self.memory]).to(self.device)\n",
        "        old_log_probs = torch.stack([m['log_prob'] for m in self.memory]).to(self.device)\n",
        "        returns, advantages = self._compute_returns_adv(self.gamma)\n",
        "        returns = returns.to(self.device)\n",
        "        advantages = advantages.to(self.device)\n",
        "        losses = []\n",
        "        for _ in range(self.epochs):\n",
        "            logits, values = self.policy(states)\n",
        "            dist = Categorical(logits=logits)\n",
        "            new_log_probs = dist.log_prob(actions)\n",
        "            entropy = dist.entropy().mean()\n",
        "            ratio = (new_log_probs - old_log_probs).exp()\n",
        "            surr1 = ratio * advantages\n",
        "            surr2 = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * advantages\n",
        "            actor_loss = -torch.min(surr1, surr2).mean()\n",
        "            critic_loss = F.mse_loss(values.squeeze(-1), returns)\n",
        "            loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "        self.memory = []\n",
        "        return {'loss': float(sum(losses) / max(len(losses), 1))}\n",
        "\n",
        "class DenseNetRefineEnv:\n",
        "    def __init__(self, train_loader, val_loader, device, base_checkpoint, class_weights=None, micro_epochs=1, max_batches_per_epoch=3):\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.device = device\n",
        "        self.base_checkpoint = Path(base_checkpoint)\n",
        "        self.class_weights = class_weights\n",
        "        self.micro_epochs = micro_epochs\n",
        "        self.max_batches = max_batches_per_epoch\n",
        "        self.actions = [\n",
        "            {'name': 'lr_up', 'lr_scale': 1.5},\n",
        "            {'name': 'lr_down', 'lr_scale': 0.7},\n",
        "            {'name': 'dropout_up', 'dropout_delta': 0.05},\n",
        "            {'name': 'dropout_down', 'dropout_delta': -0.05},\n",
        "            {'name': 'mixup_toggle', 'mixup_toggle': True},\n",
        "            {'name': 'label_smoothing_toggle', 'ls_toggle': True},\n",
        "        ]\n",
        "        self.state_dim = 5\n",
        "        self.action_dim = len(self.actions)\n",
        "        self.best_val_acc = 0.0\n",
        "        self.last_val_acc = 0.0\n",
        "        self.last_val_loss = 0.0\n",
        "        self.state = {}\n",
        "        self.model = None\n",
        "        self.reset()\n",
        "    def _build_model(self):\n",
        "        model = build_densenet(mode='classification', dropout_rate=self.state['dropout']).to(self.device)\n",
        "        if self.base_checkpoint.exists():\n",
        "            try:\n",
        "                state_dict = torch.load(self.base_checkpoint, map_location=self.device)\n",
        "                model.load_state_dict(state_dict, strict=False)\n",
        "            except Exception as e:\n",
        "                print(f\"Falha ao carregar checkpoint base: {e}\")\n",
        "        if self.state.get('freeze_backbone') and hasattr(model, 'features'):\n",
        "            for p in model.features.parameters():\n",
        "                p.requires_grad = False\n",
        "        return model\n",
        "    def reset(self):\n",
        "        self.state = {\n",
        "            'lr': 1e-4,\n",
        "            'weight_decay': 1e-4,\n",
        "            'dropout': 0.3,\n",
        "            'label_smoothing': 0.05,\n",
        "            'mixup_alpha': 0.4,\n",
        "            'freeze_backbone': False,\n",
        "            'class_balance': False,\n",
        "        }\n",
        "        self.model = self._build_model()\n",
        "        return self._as_vec()\n",
        "    def _apply_action(self, action_idx):\n",
        "        act = self.actions[action_idx]\n",
        "        if 'lr_scale' in act:\n",
        "            self.state['lr'] = float(min(5e-3, max(1e-6, self.state['lr'] * act['lr_scale'])))\n",
        "        if 'dropout_delta' in act:\n",
        "            self.state['dropout'] = float(min(0.8, max(0.05, self.state['dropout'] + act['dropout_delta'])))\n",
        "        if act.get('mixup_toggle'):\n",
        "            self.state['mixup_alpha'] = 0.0 if self.state['mixup_alpha'] > 0 else 0.4\n",
        "        if act.get('ls_toggle'):\n",
        "            self.state['label_smoothing'] = 0.0 if self.state['label_smoothing'] > 0 else 0.05\n",
        "        self.model = self._build_model()\n",
        "    def _as_vec(self):\n",
        "        return [\n",
        "            self.state['lr'],\n",
        "            self.state['weight_decay'],\n",
        "            self.state['dropout'],\n",
        "            self.state['label_smoothing'],\n",
        "            self.state['mixup_alpha'],\n",
        "        ]\n",
        "    def _run_micro_train(self, model):\n",
        "        optimizer = optim.Adam(model.parameters(), lr=self.state['lr'], weight_decay=self.state['weight_decay'])\n",
        "        criterion = torch.nn.CrossEntropyLoss(label_smoothing=self.state['label_smoothing'], weight=self.class_weights)\n",
        "        use_mixup = self.state['mixup_alpha'] > 0\n",
        "        train_loss = 0\n",
        "        total, correct = 0, 0\n",
        "        model.train()\n",
        "        for epoch in range(self.micro_epochs):\n",
        "            for batch_idx, (imgs, lbls) in enumerate(self.train_loader):\n",
        "                if batch_idx >= self.max_batches:\n",
        "                    break\n",
        "                imgs, lbls = imgs.to(self.device), lbls.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                if use_mixup:\n",
        "                    imgs_mix, targets_a, targets_b, lam = mixup_data(imgs, lbls.long(), self.state['mixup_alpha'])\n",
        "                    out = model(imgs_mix)\n",
        "                    loss = lam * criterion(out, targets_a) + (1 - lam) * criterion(out, targets_b)\n",
        "                    preds = out.argmax(dim=1)\n",
        "                    correct += (\n",
        "                        lam * (preds == targets_a).sum().item() +\n",
        "                        (1 - lam) * (preds == targets_b).sum().item()\n",
        "                    )\n",
        "                    total += lbls.size(0)\n",
        "                else:\n",
        "                    out = model(imgs)\n",
        "                    loss = criterion(out, lbls.long())\n",
        "                    preds = out.argmax(dim=1)\n",
        "                    correct += (preds == lbls.long()).sum().item()\n",
        "                    total += lbls.size(0)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item() * imgs.size(0)\n",
        "        train_loss = train_loss / max(total, 1)\n",
        "        train_acc = correct / max(total, 1)\n",
        "        return train_loss, train_acc\n",
        "    def _eval(self, model, loader):\n",
        "        model.eval()\n",
        "        criterion = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "        loss_sum = 0\n",
        "        total, correct = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (imgs, lbls) in enumerate(loader):\n",
        "                if batch_idx >= self.max_batches:\n",
        "                    break\n",
        "                imgs, lbls = imgs.to(self.device), lbls.to(self.device)\n",
        "                out = model(imgs)\n",
        "                loss = criterion(out, lbls.long())\n",
        "                preds = out.argmax(dim=1)\n",
        "                correct += (preds == lbls.long()).sum().item()\n",
        "                total += lbls.size(0)\n",
        "                loss_sum += loss.item() * imgs.size(0)\n",
        "        acc = correct / max(total, 1)\n",
        "        loss_mean = loss_sum / max(total, 1)\n",
        "        return acc, loss_mean\n",
        "    def step(self, action_idx):\n",
        "        self._apply_action(action_idx)\n",
        "        train_loss, train_acc = self._run_micro_train(self.model)\n",
        "        val_acc, val_loss = self._eval(self.model, self.val_loader)\n",
        "        self.last_val_acc = val_acc\n",
        "        self.last_val_loss = val_loss\n",
        "        reward = val_acc - 0.1 * val_loss\n",
        "        if val_acc > self.best_val_acc:\n",
        "            self.best_val_acc = val_acc\n",
        "            best_state = {k: v.cpu() for k, v in self.model.state_dict().items()}\n",
        "            self.best_state = best_state\n",
        "            self.best_hparams = dict(self.state)\n",
        "        info = {\n",
        "            'train_loss': float(train_loss),\n",
        "            'train_acc': float(train_acc),\n",
        "            'val_loss': float(val_loss),\n",
        "            'val_acc': float(val_acc),\n",
        "            'state': dict(self.state),\n",
        "            'action': self.actions[action_idx]['name'],\n",
        "        }\n",
        "        return self._as_vec(), float(reward), info\n",
        "\n",
        "def evaluate_full_model(model, loader, device):\n",
        "    model.eval()\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for imgs, lbls in loader:\n",
        "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
        "            out = model(imgs)\n",
        "            loss = criterion(out, lbls.long())\n",
        "            preds = out.argmax(dim=1)\n",
        "            correct += (preds == lbls.long()).sum().item()\n",
        "            total += lbls.size(0)\n",
        "            loss_sum += loss.item() * imgs.size(0)\n",
        "    acc = correct / max(total, 1)\n",
        "    loss_mean = loss_sum / max(total, 1)\n",
        "    return {'acc': acc, 'loss': loss_mean}\n",
        "\n",
        "def refine_densenet_with_rl(episodes=4, horizon=4, micro_epochs=1, train_subset=120, val_subset=80, base_checkpoint=None):\n",
        "    if not EXAM_SPLIT_CSV.exists():\n",
        "        raise FileNotFoundError(\"Crie o dataset (create_exam_level_dataset) antes de rodar o RL.\")\n",
        "    if base_checkpoint is None:\n",
        "        base_checkpoint = OUTPUT_DIR / \"densenet_classification.pth\"\n",
        "    df = pd.read_csv(EXAM_SPLIT_CSV)\n",
        "    df_train = df[df['split'] == 'train'].copy()\n",
        "    df_val = df[df['split'] == 'validation'].copy()\n",
        "    df_test = df[df['split'] == 'test'].copy()\n",
        "    if df_train.empty or df_val.empty:\n",
        "        raise ValueError(\"Splits de treino/valida\u00e7\u00e3o vazios para classifica\u00e7\u00e3o.\")\n",
        "\n",
        "    device = select_device()\n",
        "    train_tf, val_tf = build_transforms()\n",
        "\n",
        "    def _sample(df_split, n):\n",
        "        if len(df_split) <= n:\n",
        "            return df_split\n",
        "        return df_split.sample(n=n, random_state=42)\n",
        "\n",
        "    df_train_small = _sample(df_train, train_subset)\n",
        "    df_val_small = _sample(df_val, val_subset)\n",
        "\n",
        "    batch_small = 8\n",
        "    train_loader_small = DataLoader(\n",
        "        MRIDataset(df_train_small, train_tf, DATASET_DIR.parent, 'original_path', 'Final_Group'),\n",
        "        batch_size=batch_small, shuffle=True\n",
        "    )\n",
        "    val_loader_small = DataLoader(\n",
        "        MRIDataset(df_val_small, val_tf, DATASET_DIR.parent, 'original_path', 'Final_Group'),\n",
        "        batch_size=batch_small, shuffle=False\n",
        "    )\n",
        "\n",
        "    class_weights = None\n",
        "    class_counts = df_train['Final_Group'].value_counts()\n",
        "    if len(class_counts) >= 1:\n",
        "        total = class_counts.sum()\n",
        "        w0 = total / (2 * class_counts.get('Nondemented', max(class_counts.max(), 1)))\n",
        "        w1 = total / (2 * class_counts.get('Demented', max(class_counts.max(), 1)))\n",
        "        class_weights = torch.tensor([w0, w1], dtype=torch.float32, device=device)\n",
        "\n",
        "    env = DenseNetRefineEnv(\n",
        "        train_loader=train_loader_small,\n",
        "        val_loader=val_loader_small,\n",
        "        device=device,\n",
        "        base_checkpoint=base_checkpoint,\n",
        "        class_weights=class_weights,\n",
        "        micro_epochs=micro_epochs,\n",
        "        max_batches_per_epoch=3,\n",
        "    )\n",
        "    agent = PPOAgent(state_dim=env.state_dim, action_dim=env.action_dim, device=device)\n",
        "    history = {\"episodes\": [], \"actions\": env.actions}\n",
        "\n",
        "    state = env.reset()\n",
        "    for ep in range(episodes):\n",
        "        ep_reward = 0.0\n",
        "        ep_steps = []\n",
        "        for _ in range(horizon):\n",
        "            action_idx, log_prob, value_est = agent.select_action(state)\n",
        "            next_state, reward, info = env.step(action_idx)\n",
        "            agent.store(state, action_idx, log_prob, value_est, reward, done=False)\n",
        "            ep_reward += reward\n",
        "            ep_steps.append(info)\n",
        "            state = next_state\n",
        "        update_stats = agent.update()\n",
        "        history[\"episodes\"].append({\n",
        "            \"episode\": ep + 1,\n",
        "            \"reward_sum\": float(ep_reward),\n",
        "            \"last_val_acc\": float(env.last_val_acc),\n",
        "            \"last_val_loss\": float(env.last_val_loss),\n",
        "            \"best_val_acc\": float(env.best_val_acc),\n",
        "            \"steps\": ep_steps,\n",
        "            \"update\": update_stats,\n",
        "        })\n",
        "        state = env.reset()\n",
        "\n",
        "    best_state = getattr(env, 'best_state', None)\n",
        "    best_hparams = getattr(env, 'best_hparams', None)\n",
        "    if best_state is None:\n",
        "        best_state = {k: v.cpu() for k, v in env.model.state_dict().items()}\n",
        "        best_hparams = env.state\n",
        "\n",
        "    eval_model = env._build_model()\n",
        "    eval_model.load_state_dict(best_state, strict=False)\n",
        "    eval_model = eval_model.to(device)\n",
        "\n",
        "    val_loader_full = DataLoader(\n",
        "        MRIDataset(df_val, val_tf, DATASET_DIR.parent, 'original_path', 'Final_Group'),\n",
        "        batch_size=16, shuffle=False\n",
        "    )\n",
        "    test_loader_full = DataLoader(\n",
        "        MRIDataset(df_test, val_tf, DATASET_DIR.parent, 'original_path', 'Final_Group'),\n",
        "        batch_size=16, shuffle=False\n",
        "    )\n",
        "\n",
        "    val_metrics = evaluate_full_model(eval_model, val_loader_full, device)\n",
        "    test_metrics = evaluate_full_model(eval_model, test_loader_full, device)\n",
        "\n",
        "    def _collect_preds(loader):\n",
        "        eval_model.eval()\n",
        "        y_true, y_pred = [], []\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in loader:\n",
        "                imgs = imgs.to(device)\n",
        "                labels = labels.to(device).long()\n",
        "                logits = eval_model(imgs)\n",
        "                preds = logits.argmax(dim=1)\n",
        "                y_true.append(labels.cpu().numpy())\n",
        "                y_pred.append(preds.cpu().numpy())\n",
        "        if not y_true:\n",
        "            return np.array([]), np.array([])\n",
        "        return np.concatenate(y_true), np.concatenate(y_pred)\n",
        "\n",
        "    y_val, y_val_pred = _collect_preds(val_loader_full)\n",
        "    y_test, y_test_pred = _collect_preds(test_loader_full)\n",
        "    val_cm = confusion_matrix(y_val, y_val_pred) if y_val.size else None\n",
        "    test_cm = confusion_matrix(y_test, y_test_pred) if y_test.size else None\n",
        "\n",
        "    best_model_path = OUTPUT_DIR / \"densenet_classification_rl_best.pth\"\n",
        "    torch.save(best_state, best_model_path)\n",
        "    policy_path = OUTPUT_DIR / \"rl_policy_densenet.pth\"\n",
        "    torch.save(agent.policy.state_dict(), policy_path)\n",
        "\n",
        "    history_writer = TrainHistoryWriter(OUTPUT_DIR)\n",
        "    history[\"meta\"] = {\n",
        "        \"episodes\": episodes,\n",
        "        \"horizon\": horizon,\n",
        "        \"micro_epochs\": micro_epochs,\n",
        "        \"train_subset\": len(df_train_small),\n",
        "        \"val_subset\": len(df_val_small),\n",
        "        \"best_val_acc\": float(env.best_val_acc),\n",
        "        \"base_checkpoint\": Path(base_checkpoint).name,\n",
        "    }\n",
        "    history_file = history_writer.save(history)\n",
        "\n",
        "    exp_payload = {\n",
        "        'model': 'DenseNet_classification_RL',\n",
        "        'episodes': episodes,\n",
        "        'horizon': horizon,\n",
        "        'micro_epochs': micro_epochs,\n",
        "        'train_subset': len(df_train_small),\n",
        "        'val_subset': len(df_val_small),\n",
        "        'best_val_acc': float(env.best_val_acc),\n",
        "        'val_accuracy': float(val_metrics.get(\"acc\", 0.0)),\n",
        "        'test_accuracy': float(test_metrics.get(\"acc\", 0.0)),\n",
        "        'val_loss': float(val_metrics.get(\"loss\", 0.0)),\n",
        "        'test_loss': float(test_metrics.get(\"loss\", 0.0)),\n",
        "        'history_file': history_file.name,\n",
        "        'best_model_path': best_model_path.name,\n",
        "        'policy_path': policy_path.name,\n",
        "        'best_hparams': best_hparams,\n",
        "    }\n",
        "    if val_cm is not None:\n",
        "        exp_payload['val_confusion_matrix'] = val_cm.tolist()\n",
        "        exp_payload['val_classes'] = ['Nondemented', 'Demented']\n",
        "    if test_cm is not None:\n",
        "        exp_payload['test_confusion_matrix'] = test_cm.tolist()\n",
        "        exp_payload['test_classes'] = ['Nondemented', 'Demented']\n",
        "\n",
        "    if best_hparams:\n",
        "        try:\n",
        "            print(\"Treinando DenseNet final com hiperpar\u00e2metros do RL (split completo)...\")\n",
        "            train_densenet(mode='classification', hparams=best_hparams)\n",
        "        except Exception as e:\n",
        "            print(f\"Falha ao treinar modelo final com hparams do RL: {e}\")\n",
        "\n",
        "    save_experiment(exp_payload)\n",
        "    print(\"RL conclu\u00eddo.\", exp_payload)\n",
        "    return exp_payload\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee7aa0a8",
      "metadata": {},
      "source": [
        "### Visualiza\u00e7\u00f5es offline (heatmap, scatterplots, t-SNE/UMAP)\n",
        "\n",
        "Fun\u00e7\u00f5es para gerar gr\u00e1ficos sem Tkinter: scatterplots de descritores, heatmap de correla\u00e7\u00e3o e redu\u00e7\u00f5es 2D de embeddings (t-SNE/UMAP)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc99e201",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiza\u00e7\u00f5es offline: scatterplots de descritores, heatmap de correla\u00e7\u00e3o, t-SNE/UMAP de embeddings.\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "try:\n",
        "    import umap  # type: ignore\n",
        "    UMAP_AVAILABLE = True\n",
        "except ImportError:\n",
        "    UMAP_AVAILABLE = False\n",
        "\n",
        "def _scatter_by_group(df, x_col, y_col, hue_col, title, out_path):\n",
        "    plot_df = df.dropna(subset=[x_col, y_col])\n",
        "    if plot_df.empty:\n",
        "        return None\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    if pd.api.types.is_numeric_dtype(plot_df[hue_col]) and plot_df[hue_col].nunique() > 10:\n",
        "        scatter = ax.scatter(plot_df[x_col], plot_df[y_col], c=plot_df[hue_col], cmap='viridis', alpha=0.8, s=40)\n",
        "        cbar = fig.colorbar(scatter, ax=ax)\n",
        "        cbar.set_label(hue_col)\n",
        "    else:\n",
        "        sns.scatterplot(data=plot_df, x=x_col, y=y_col, hue=hue_col, palette='tab10', alpha=0.85, s=50, edgecolor='white', linewidth=0.3, ax=ax)\n",
        "        ax.legend(title=hue_col)\n",
        "    ax.set_xlabel(x_col)\n",
        "    ax.set_ylabel(y_col)\n",
        "    ax.set_title(title)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(out_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    return out_path\n",
        "\n",
        "def generate_descriptor_scatterplots(limit_pairs=None, hue_col='Group'):\n",
        "    if not DESCRIPTORS_CSV.exists():\n",
        "        raise FileNotFoundError(f\"Descritores n\u00e3o encontrados: {DESCRIPTORS_CSV}\")\n",
        "    if not CSV_DEMOGRAPHIC.exists():\n",
        "        raise FileNotFoundError(f\"CSV demogr\u00e1fico n\u00e3o encontrado: {CSV_DEMOGRAPHIC}\")\n",
        "\n",
        "    df_desc = pd.read_csv(DESCRIPTORS_CSV)\n",
        "    df_demo = pd.read_csv(CSV_DEMOGRAPHIC, sep=';', decimal=',')\n",
        "    df_demo = df_demo.rename(columns=lambda x: x.strip())\n",
        "    if 'MRI ID' in df_demo.columns:\n",
        "        df_demo = df_demo.rename(columns={'MRI ID': 'MRI_ID'})\n",
        "    merged = pd.merge(df_desc, df_demo[[c for c in df_demo.columns if c in ['MRI_ID', hue_col]]], on='MRI_ID', how='left')\n",
        "    if hue_col not in merged.columns:\n",
        "        raise ValueError(f\"Coluna de agrupamento '{hue_col}' n\u00e3o encontrada ap\u00f3s merge.\")\n",
        "\n",
        "    descriptor_cols = [c for c in df_desc.columns if c not in ['viable', 'MRI_ID', 'Subject_ID', 'segmented_path']]\n",
        "    descriptor_cols = [c for c in descriptor_cols if merged[c].dtype.kind in 'biufc']\n",
        "    pairs = []\n",
        "    for i in range(len(descriptor_cols)):\n",
        "        for j in range(i + 1, len(descriptor_cols)):\n",
        "            pairs.append((descriptor_cols[i], descriptor_cols[j]))\n",
        "    if limit_pairs:\n",
        "        pairs = pairs[:limit_pairs]\n",
        "\n",
        "    out_paths = []\n",
        "    for x_col, y_col in pairs:\n",
        "        merged[x_col] = pd.to_numeric(merged[x_col], errors='coerce')\n",
        "        merged[y_col] = pd.to_numeric(merged[y_col], errors='coerce')\n",
        "        title = f\"{y_col} vs {x_col}\"\n",
        "        out_path = OUTPUT_DIR / f\"scatter_{x_col}_vs_{y_col}.png\"\n",
        "        p = _scatter_by_group(merged, x_col, y_col, hue_col, title, out_path)\n",
        "        if p:\n",
        "            out_paths.append(p)\n",
        "    print(f\"Scatterplots gerados: {len(out_paths)} em {OUTPUT_DIR}\")\n",
        "    return out_paths\n",
        "\n",
        "def plot_correlation_heatmap(split_path=EXAM_SPLIT_CSV, out_path=None):\n",
        "    split_path = Path(split_path)\n",
        "    out_path = Path(out_path) if out_path else OUTPUT_DIR / \"correlation_heatmap.png\"\n",
        "    if not split_path.exists():\n",
        "        raise FileNotFoundError(f\"Crie o dataset combinado primeiro: {split_path}\")\n",
        "    df = pd.read_csv(split_path)\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_cols = [c for c in numeric_cols if c not in ['converted']]\n",
        "    if not numeric_cols:\n",
        "        raise ValueError(\"Nenhuma coluna num\u00e9rica para correlacionar.\")\n",
        "\n",
        "    df_numeric = df[numeric_cols].dropna(axis=1, how='all')\n",
        "    if 'Final_Group' in df.columns:\n",
        "        class_dummies = pd.get_dummies(df['Final_Group'], prefix='Class')\n",
        "        df_numeric = pd.concat([df_numeric, class_dummies], axis=1)\n",
        "\n",
        "    corr = df_numeric.corr()\n",
        "    n_features = len(corr.columns)\n",
        "    fig_size = max(8, min(20, n_features * 0.6))\n",
        "    fig, ax = plt.subplots(figsize=(fig_size, fig_size))\n",
        "    sns.heatmap(corr, cmap='coolwarm', center=0, square=True, ax=ax, cbar_kws={'shrink': 0.5})\n",
        "    ax.set_title('Heatmap de Correla\u00e7\u00e3o')\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(out_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    print(f\"Heatmap salvo em {out_path}\")\n",
        "    return out_path\n",
        "\n",
        "def run_tsne_umap(emb_path, target_col='target', out_prefix=None):\n",
        "    emb_path = Path(emb_path)\n",
        "    out_prefix = Path(out_prefix) if out_prefix else OUTPUT_DIR / emb_path.stem\n",
        "    if not emb_path.exists():\n",
        "        raise FileNotFoundError(f\"CSV de embeddings n\u00e3o encontrado: {emb_path}\")\n",
        "\n",
        "    df = pd.read_csv(emb_path)\n",
        "    if target_col not in df.columns:\n",
        "        raise ValueError(f\"Coluna alvo '{target_col}' n\u00e3o encontrada no CSV.\")\n",
        "    meta_cols = {'MRI_ID', target_col}\n",
        "    feature_cols = [c for c in df.columns if c not in meta_cols]\n",
        "    if not feature_cols:\n",
        "        raise ValueError(\"Nenhuma coluna de feature encontrada.\")\n",
        "\n",
        "    X = df[feature_cols].values\n",
        "    y = df[target_col].values\n",
        "    X_scaled = StandardScaler().fit_transform(X)\n",
        "    n_samples = len(X_scaled)\n",
        "    perplexity = max(5, min(30, n_samples - 1))\n",
        "\n",
        "    print(f\"Executando t-SNE (perplexity={perplexity}) para {n_samples} amostras...\")\n",
        "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, init='pca')\n",
        "    X_tsne = tsne.fit_transform(X_scaled)\n",
        "    df_tsne = pd.DataFrame({'x': X_tsne[:, 0], 'y': X_tsne[:, 1], 'target': y})\n",
        "    tsne_path = out_prefix.with_suffix('.tsne.png')\n",
        "    _scatter_by_group(df_tsne, 'x', 'y', 'target', 't-SNE dos embeddings', tsne_path)\n",
        "\n",
        "    if UMAP_AVAILABLE:\n",
        "        print(\"Executando UMAP...\")\n",
        "        reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "        X_umap = reducer.fit_transform(X_scaled)\n",
        "        df_umap = pd.DataFrame({'x': X_umap[:, 0], 'y': X_umap[:, 1], 'target': y})\n",
        "        umap_path = out_prefix.with_suffix('.umap.png')\n",
        "        _scatter_by_group(df_umap, 'x', 'y', 'target', 'UMAP dos embeddings', umap_path)\n",
        "    else:\n",
        "        print(\"UMAP n\u00e3o dispon\u00edvel (instale 'umap-learn' para habilitar).\")\n",
        "\n",
        "    return out_prefix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c447517c",
      "metadata": {},
      "source": [
        "### Exemplos de uso (execute conforme necessidade)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "035c3f1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 1) Segmentar (ajuste limit para um dry-run r\u00e1pido)\n",
        "# segment_all_images(limit=2, overwrite=False)\n",
        "\n",
        "# 2) Criar dataset combinado\n",
        "# merged = create_exam_level_dataset()\n",
        "# display(merged.head())\n",
        "\n",
        "# 3) Treinos cl\u00e1ssicos\n",
        "# svm_metrics = train_svm_classifier()\n",
        "# xgb_metrics = train_xgboost_regressor()\n",
        "\n",
        "# 4) DenseNet (reduza max_epochs para teste r\u00e1pido)\n",
        "# densenet_cls = train_densenet(mode='classification', max_epochs=2)\n",
        "# densenet_reg = train_densenet(mode='regression', max_epochs=2)\n",
        "\n",
        "\n",
        "# 5) Visualiza\u00e7\u00f5es\n",
        "# generate_descriptor_scatterplots(limit_pairs=12)\n",
        "# plot_correlation_heatmap()\n",
        "# run_tsne_umap('output/densenet_embeddings_classification_test.csv', target_col='target')\n",
        "\n",
        "# 6) Refinamento via RL (usa checkpoint de classifica\u00e7\u00e3o)\n",
        "# refine_densenet_with_rl(episodes=2, horizon=2, micro_epochs=1, train_subset=40, val_subset=30)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}